{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/achrafwali/polynomial-regression-feature-engineering?scriptVersionId=194110933\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-13T13:16:31.138027Z","iopub.execute_input":"2024-08-13T13:16:31.138356Z","iopub.status.idle":"2024-08-13T13:16:31.149564Z","shell.execute_reply.started":"2024-08-13T13:16:31.138331Z","shell.execute_reply":"2024-08-13T13:16:31.148584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Tools","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-13T15:43:03.879142Z","iopub.execute_input":"2024-08-13T15:43:03.879532Z","iopub.status.idle":"2024-08-13T15:43:04.672902Z","shell.execute_reply.started":"2024-08-13T15:43:03.879496Z","shell.execute_reply":"2024-08-13T15:43:04.671659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the target data: x is a range from 0 to 19, and y is generated by applying the function y = 1 + x^2\nx = np.arange(0, 20, 1)\ny = 1 + x**2\n# Reshape x to be a 2D array for matrix operations\nX = x.reshape(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T14:25:00.04729Z","iopub.execute_input":"2024-08-13T14:25:00.047664Z","iopub.status.idle":"2024-08-13T14:25:00.052967Z","shell.execute_reply.started":"2024-08-13T14:25:00.047637Z","shell.execute_reply":"2024-08-13T14:25:00.051778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model & Gradient_descent","metadata":{}},{"cell_type":"code","source":"# Define the model: a linear model where X is multiplied by the parameter vector theta\ndef model(X, theta):\n    return X.dot(theta)\n# Compute the gradient of the cost function with respect to theta\ndef grad(X, y, theta):\n    m = len(y)\n    return 1/m * X.T.dot(model(X, theta) - y)\n# Perform gradient descent to optimize theta\ndef gradient_descent(X, y, theta, learning_rate, n_iterations):    \n    # Iterate over the number of iterations with a progress bar to track the iteration progress\n    for i in tqdm (range(0, n_iterations) ):\n        theta = theta - learning_rate * grad(X, y, theta)\n    return theta","metadata":{"execution":{"iopub.status.busy":"2024-08-13T15:45:53.442273Z","iopub.execute_input":"2024-08-13T15:45:53.442674Z","iopub.status.idle":"2024-08-13T15:45:53.449565Z","shell.execute_reply.started":"2024-08-13T15:45:53.442627Z","shell.execute_reply":"2024-08-13T15:45:53.448384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seed the random number generator for reproducibility\nnp.random.seed(0)\n# Initialize theta as a random 2x1 vector (this represents the weights and bias)\n# Here, theta is the vector of parameters (weights and bias) for the model\n# The model is defined as F(x) = w0 * x0 + ... + wn * xn + b, where F(x) = X * theta\ntheta =  np.random.randn(2, 1)\n# Add a column of ones to X to account for the bias term in the model\n# This allows the bias to be included in the linear model as an additional parameter\nX = np.hstack((X, np.ones( (X.shape[0],1) )))\n# Display the resulting matrix X\nX","metadata":{"execution":{"iopub.status.busy":"2024-08-13T14:25:05.783083Z","iopub.execute_input":"2024-08-13T14:25:05.783486Z","iopub.status.idle":"2024-08-13T14:25:05.795157Z","shell.execute_reply.started":"2024-08-13T14:25:05.783457Z","shell.execute_reply":"2024-08-13T14:25:05.793893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### No feature engineering","metadata":{}},{"cell_type":"code","source":"# Run gradient_descent\nn_iterations = 10000\nlearning_rate = 0.00001\ntheta_final= gradient_descent(X, y.reshape(-1,1), theta, learning_rate, n_iterations)\n# Display the resulting theta_final\ntheta_final","metadata":{"execution":{"iopub.status.busy":"2024-08-13T14:25:09.155813Z","iopub.execute_input":"2024-08-13T14:25:09.156232Z","iopub.status.idle":"2024-08-13T14:25:09.247748Z","shell.execute_reply.started":"2024-08-13T14:25:09.156184Z","shell.execute_reply":"2024-08-13T14:25:09.246516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the actual data points as red 'x' markers\nplt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"no feature engineering\")\n# Plot the predicted values from the model as a line plot\nplt.plot(x, model(X, theta_final) , label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T14:25:15.628358Z","iopub.execute_input":"2024-08-13T14:25:15.629353Z","iopub.status.idle":"2024-08-13T14:25:15.912263Z","shell.execute_reply.started":"2024-08-13T14:25:15.629313Z","shell.execute_reply":"2024-08-13T14:25:15.911238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well,  not a great fit. What is needed is something like  ùë¶=ùë§0 ùë•**2 +ùëè or a polynomial feature.","metadata":{}},{"cell_type":"markdown","source":"## with feature engineering","metadata":{}},{"cell_type":"markdown","source":"## polynomial feature\n","metadata":{}},{"cell_type":"code","source":"# create target data\nx = np.arange(0, 20, 1)\ny = 1 + x**2\n# Engineer features \nX = x**2      #<-- added engineered feature\nX = X.reshape(-1, 1)  #X should be a 2-D Matrix","metadata":{"execution":{"iopub.status.busy":"2024-08-13T13:57:52.874949Z","iopub.execute_input":"2024-08-13T13:57:52.875388Z","iopub.status.idle":"2024-08-13T13:57:52.880879Z","shell.execute_reply.started":"2024-08-13T13:57:52.875355Z","shell.execute_reply":"2024-08-13T13:57:52.879795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(0)\ntheta =  np.random.randn(2, 1)\nX = np.hstack((X, np.ones( (X.shape[0],1) )))\nprint (theta)\nprint (X)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T13:57:54.637259Z","iopub.execute_input":"2024-08-13T13:57:54.637651Z","iopub.status.idle":"2024-08-13T13:57:54.645018Z","shell.execute_reply.started":"2024-08-13T13:57:54.63762Z","shell.execute_reply":"2024-08-13T13:57:54.643944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run gradient_descent\nn_iterations = 10000\nlearning_rate = 0.00001\ntheta_final= gradient_descent(X, y.reshape(-1,1), theta, learning_rate, n_iterations)\ntheta_final","metadata":{"execution":{"iopub.status.busy":"2024-08-13T13:53:30.144079Z","iopub.execute_input":"2024-08-13T13:53:30.145075Z","iopub.status.idle":"2024-08-13T13:53:30.225003Z","shell.execute_reply.started":"2024-08-13T13:53:30.145031Z","shell.execute_reply":"2024-08-13T13:53:30.224003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Added x**2 feature\")\nplt.plot(x, model(X, theta_final) , label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T13:53:34.661108Z","iopub.execute_input":"2024-08-13T13:53:34.661517Z","iopub.status.idle":"2024-08-13T13:53:34.956244Z","shell.execute_reply.started":"2024-08-13T13:53:34.661479Z","shell.execute_reply":"2024-08-13T13:53:34.955303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Gradient descent modified our initial values of  ùê∞,ùëè\n  to be (1.002,0.0423) or a model of  ùë¶ = 1 ‚àó ùë•** 2 + 0.042\n , very close to our target of  ùë¶= 1 ‚àó ùë•** 2 + 1\n . If we ran it longer, it could be a better match.","metadata":{}},{"cell_type":"markdown","source":"### Selecting Features\n","metadata":{}},{"cell_type":"markdown","source":"Above, we knew that an $x^2$ term was required. It may not always be obvious which features are required. One could add a variety of potential features to try and find the most useful. For example, what if we had instead tried : $y=w_0x_0 + w_1x_1^2 + w_2x_2^3+b$ ? \n","metadata":{}},{"cell_type":"code","source":"# create target data\nx = np.arange(0, 20, 1)\ny = x**2\n# engineer features .\nX = np.c_[x, x**2, x**3]   #<-- added engineered feature","metadata":{"execution":{"iopub.status.busy":"2024-08-13T15:46:30.563348Z","iopub.execute_input":"2024-08-13T15:46:30.563786Z","iopub.status.idle":"2024-08-13T15:46:30.569502Z","shell.execute_reply.started":"2024-08-13T15:46:30.56375Z","shell.execute_reply":"2024-08-13T15:46:30.568348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(4)\ntheta =  np.random.randn(4, 1)\nX = np.hstack((X, np.ones( (X.shape[0],1) )))\nprint (theta)\nprint (X)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T14:29:07.74028Z","iopub.execute_input":"2024-08-13T14:29:07.740655Z","iopub.status.idle":"2024-08-13T14:29:07.747888Z","shell.execute_reply.started":"2024-08-13T14:29:07.740629Z","shell.execute_reply":"2024-08-13T14:29:07.746733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run gradient_descent\nn_iterations = 1000000\nlearning_rate = 1e-7\ntheta_final= gradient_descent(X, y.reshape(-1,1), theta, learning_rate, n_iterations)\ntheta_final","metadata":{"execution":{"iopub.status.busy":"2024-08-13T14:29:08.261437Z","iopub.execute_input":"2024-08-13T14:29:08.261796Z","iopub.status.idle":"2024-08-13T14:29:15.963067Z","shell.execute_reply.started":"2024-08-13T14:29:08.261771Z","shell.execute_reply":"2024-08-13T14:29:15.962047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"x, x**2, x**3 features\")\nplt.plot(x, model(X, theta_final) , label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T15:55:29.47402Z","iopub.execute_input":"2024-08-13T15:55:29.47445Z","iopub.status.idle":"2024-08-13T15:55:29.796098Z","shell.execute_reply.started":"2024-08-13T15:55:29.474419Z","shell.execute_reply":"2024-08-13T15:55:29.794693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note the value of $\\mathbf{w}$, `[0.074 0.974 0.001]` and b is `0.679`.This implies the model after fitting/training is:\n$$ 0.08x + 0.9x^2 + 0.001x^3 + 0.679 $$ \nGradient descent has highlighted the data that aligns most closely with the $x^2$ data by increasing the influence of the $w_1$term relative to the others. If the process were to continue for an extended period, it would progressively minimize the influence of the other terms.","metadata":{}},{"cell_type":"markdown","source":"## Scaling Features\n\n\nApplying feature scaling is essential when a dataset contains features with significantly different scales, as it accelerates gradient descent. In our current example, the features $x$, $x^2$, and $x^3$ inherently vary in scale. To address this, we'll implement Z-score normalization.","metadata":{}},{"cell_type":"code","source":"# create target data\nx = np.arange(0,20,1)\nX = np.c_[x, x**2, x**3]\ny = x**2\nprint(f\"Peak to Peak range by column in Raw        X:{np.ptp(X,axis=0)}\")\n\n# add mean_normalization \nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nprint(f\"Peak to Peak range by column in Normalized X:{np.ptp(X,axis=0)}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T15:50:57.404895Z","iopub.execute_input":"2024-08-13T15:50:57.405268Z","iopub.status.idle":"2024-08-13T15:50:57.416586Z","shell.execute_reply.started":"2024-08-13T15:50:57.405239Z","shell.execute_reply":"2024-08-13T15:50:57.415194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(8)\ntheta =  np.random.randn(4, 1)\nX = np.hstack((X, np.ones( (X.shape[0],1) )))\nprint (theta)\nprint (X)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T15:51:00.302798Z","iopub.execute_input":"2024-08-13T15:51:00.303198Z","iopub.status.idle":"2024-08-13T15:51:00.311513Z","shell.execute_reply.started":"2024-08-13T15:51:00.303158Z","shell.execute_reply":"2024-08-13T15:51:00.310235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can try again with a more aggressive value of alpha:","metadata":{}},{"cell_type":"code","source":"# Run gradient_descent\nn_iterations = 1000000\nlearning_rate = 1e-1\ntheta_final= gradient_descent(X, y.reshape(-1,1), theta, learning_rate, n_iterations)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T15:51:03.130194Z","iopub.execute_input":"2024-08-13T15:51:03.130581Z","iopub.status.idle":"2024-08-13T15:51:11.040781Z","shell.execute_reply.started":"2024-08-13T15:51:03.130549Z","shell.execute_reply":"2024-08-13T15:51:11.039708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Normalized x x**2, x**3 feature\")\nplt.plot(x, model(X, theta_final) , label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T15:54:38.006631Z","iopub.execute_input":"2024-08-13T15:54:38.007152Z","iopub.status.idle":"2024-08-13T15:54:38.406168Z","shell.execute_reply.started":"2024-08-13T15:54:38.007118Z","shell.execute_reply":"2024-08-13T15:54:38.405031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Complex Functions\nThrough feature engineering, it becomes possible to model even highly complex functions.\n\n","metadata":{}},{"cell_type":"code","source":"x = np.arange(0,20,1)\ny = np.cos(x/2)\n\nX = np.c_[x, x**2, x**3,x**4, x**5, x**6, x**7, x**8, x**9, x**10, x**11, x**12, x**13]\n\n# add mean_normalization \nscaler = StandardScaler()\nX = scaler.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T16:01:22.065041Z","iopub.execute_input":"2024-08-13T16:01:22.065431Z","iopub.status.idle":"2024-08-13T16:01:22.073834Z","shell.execute_reply.started":"2024-08-13T16:01:22.0654Z","shell.execute_reply":"2024-08-13T16:01:22.072735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(8)\ntheta =  np.random.randn(14, 1)\nX = np.hstack((X, np.ones( (X.shape[0],1) )))\n\n# Run gradient_descent\nn_iterations = 1000000\nlearning_rate = 1e-1\ntheta_final= gradient_descent(X, y.reshape(-1,1), theta, learning_rate, n_iterations)\n\nplt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Normalized x x**2, x**3 feature\")\nplt.plot(x, model(X, theta_final) , label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T16:01:23.949936Z","iopub.execute_input":"2024-08-13T16:01:23.950368Z","iopub.status.idle":"2024-08-13T16:01:30.831557Z","shell.execute_reply.started":"2024-08-13T16:01:23.950335Z","shell.execute_reply":"2024-08-13T16:01:30.830126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}